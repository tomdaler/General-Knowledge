

Regresion lineal, salida continua
    Supervised
    Features continua
    Salida   continua


Time Series
    Superised
    Regresion
    Features continua
    Salida Continua
    Particularidad: Tendencia / Season / Ruido


Logistica: regresion
    Supervised
    Features continua
    Salida   discreta

Arbol de decision, salida discreta como logistica
    Supervised
    Features continua (hombre/mujer , joven/ adulto) para tomar decisiones
    Salida   discreta

No se ve una regresion, sino que se evalua como decisiones


Random Forest
    Supervisado
    Se producen muchos arboles de decision con la misma data
    Se crea consenso de las diferentes salidas

    1) RandomForestClassification : output en categorias
    2) RandomForestRegression: output continuo
                

SVM
   Supervisado
   Ver clusters
   Output discreto

#cross_val_score(svm.SVC(kernel='linear',C=10,gamma='auto'),iris.data, iris.target, cv=5)
#cross_val_score(svm.SVC(kernel='rbf',C=10,gamma='auto'),iris.data, iris.target, cv=5)  for no linear, complex grupos, like a circle inside another circle


KNN
   Supervisado
   Se tiene clusters
   Un punto confuso entre varios clusters evalua a quien es mas cercano


Matriz confusion ----
   Output Categorias, 4 - 20


Naive Bayes
   Similar a Regresion, pero no se puede apreciar regresion
   Supervisado
   Features continuos
   Output: DISCRETO <--- IMP
   Features independientes entre si <--
   Para mas features pero muchos features pueden generar colinealidad
   Text classification, recommendation, filter spam <-- x probabilidad

   Options: 
	Berboulli: feaures are binary
	Multinomial: features discrete, category
	Gaussian: features continuous

Pros
•	It is easy to implement and fast.
•	It will converge faster than discriminative models like logistic regression.
•	It requires less training data.
•	It is highly scalable in nature, or they scale linearly with the number of predictors and data points.
•	It can make probabilistic predictions and can handle continuous as well as discrete data.
•	Can be used for binary as well as multi-class classification problems both.

Cons
•	Need feature independence, difficult when features increase.
•	Its ‘zero frequency’ which means that if a categorial variable has a category but not being observed in training data set, then Naïve Bayes model will assign a zero probability to it and it will be unable to make a prediction.

 --> CON BAYES, NO HE VISTO QUE SEA NECESARIO SCALE O NORMALIZE <--


K-means, K-median, DBScan, Hierarchical (dendrograma)
    No supervisado
    Para crear clusters

