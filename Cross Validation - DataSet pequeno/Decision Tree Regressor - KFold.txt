
LO QUE HAREMOS:
EL SIMPLE
Y LUEGO CON KFOLD  PARTICIONES


#================================
# kfold SEPARA TODA LA DATA EN n GRUPOS RANDOM
#================================


ENCONTRAREMOS MSE, ERROR DE (Y2 - Y1)2


import pandas as pd
import numpy as np

from sklearn.metrics import mean_squared_error

from sklearn.tree import DecisionTreeRegressor

from sklearn.model_selection import (
   cross_val_score, KFold
)

dataset = pd.read_csv('felicidad.csv')
X = dataset.drop(['country', 'score'], axis=1)
y = dataset['score']
    
model = DecisionTreeRegressor()

score = cross_val_score(model, X,y, cv=5, scoring='neg_mean_squared_error')
print ("MSE para cv=5, 5 valores")
print(score)
print(np.abs(np.mean(score)))

print("")

#================================
# SEPARA TODA LA DATA EN 3 GRUPOS
#================================

kf = KFold(n_splits=3, shuffle=True, random_state=42)
mse_values = []


for train, test in kf.split(dataset):
    X_train = X.iloc[train]
    X_test = X.iloc[test]

    y_train = y[train]
    y_test = y[test]


    model = DecisionTreeRegressor().fit(X_train,y_train)
    predict = model.predict(X_test)
    mse = mean_squared_error(y_test,predict)
    mse_values.append(mse)

    #model.fit(X_train, y_train)
    #predictions = model.predict(X_test)

    #.append(mean_squared_error(y_test,predict))
 
print('Error para cada partición: ', mse_values)
print('Promedio de los errores: ', np.mean(mse_values))



MSE para cv=5, 5 valores
[-0.41918385 -0.0690488  -0.04725946 -0.07705145 -0.42038598]
0.20658590739938662

Error para cada partición:  [0.023263823850418957, 0.00607798134184287, 0.004334877961562798]
Promedio de los errores:  0.011225561051274875


