With a Leaky ReLU (LReLU), you won’t face the “dead ReLU” (or “dying ReLU”) problem 
which happens when your ReLU always have values under 0 - this completely blocks learning in the ReLU because of gradients of 0 in the negative part. So:


ReLU: The derivative of the ReLU is 1 in the positive part, and 0 in the negative part.
LReLU: The derivative of the LReLU is 1 in the positive part, and is a small fraction in the negative part.

Now, think about the chain rule in the backward pass. 

If the derivative of the slope of the ReLU is of 0, absolutely no learning is performed on the layers below the dead ReLU, 
because 0 will be multiplied to the accumulated gradient for the weight update. 
Thus, you can have dead neurons. 
This problem doesn’t happen with LReLU or ELU for example, 
they will always have a little slope to allow the gradients to flow on.


An advantage of using a LReLU is thus that you can worry less about the initialization of your neural network. 
In the ReLU case, you can end up with a neural network that never learns if the neurons are not activated at the start. 
You may have lots of dead ReLU without even knowing. 

However, ReLU computes faster and can introduce a sort of “optimal brain damage” regularization in your machine learning algorithm.