
Antes las funciones de activación sigmoide y tanh eran preferidas
Hoy en día, se observa un mejor rendimiento utilizando la función de activación relu, en capas que no sean la ultima. 

Capas internas tienen buen rendimiento con funcion activacion relu

Cuando la salida es binaria (0,1) 
- Ultima capa con activacion sigmoid  (obliga a salida binaria 0,1)
- loss = 'binary_crossentropy'

Cuando la salida es 0-9 (10 categorias)
- Ultima capa con activacion softmax
- loss = 'categorical_crossentropy'

Si la salida es -1,1 
- Ultima capa con activacion tanh  (obliga a salida -1 o 1)
- loss = 'binary_crossentropy'
 
