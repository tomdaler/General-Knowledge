Dilución (también conocido como Dropout) es una técnica de regularización para reducir el sobreajuste en redes neuronales artificiales. 

Es una forma eficiente de realizar promedios de modelos con redes neuronales. 

El término dropout significa "abandonar" u omitir aleatoriamente neuronas(tanto ocultas como visibles) durante el proceso de entrenamiento de una red neuronal.

Tanto la reducción de los pesos como omitir unidades obtienen el mismo tipo de regularización.


EL PROBLEMA QUE RESUELVE

Cuando red se vuelve compleja, con muchos datos de entrenamiento
Empieza a mejorizar, y se vuelve buena con datos de entrenamiento
Luego con los datos de test falla, es OVERFITTING

Las capas las elimina, desconecta, no las multiplica todas x todas
Hace aleatoriedad para conectarlas


